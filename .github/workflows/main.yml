name: Scrape, Update, and Deploy

on:
  schedule:
    - cron: "0 * * * *"  # Every hour
  workflow_dispatch:
    inputs:
      force_deploy:
        description: 'Force deploy to Pages (skip scrape/update)'
        type: boolean
        default: false

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: "scrape-update-deploy"
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    outputs:
      data_changed: ${{ steps.commit.outputs.data_changed }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Fetch and Aggregate Data
        env:
          URL_TO_SCRAPE: ${{ secrets.URL_TO_SCRAPE }}
          API_KEY: ${{ secrets.API_KEY }}
        run: |
          set -e
          if [ -z "${URL_TO_SCRAPE}" ] || [ -z "${API_KEY}" ]; then
            echo "Secrets missing!"; exit 1;
          fi

          LIMIT=100
          OFFSET=0
          PAGE_FILE="page.json"
          HEADERS_FILE="headers.txt"
          TEMP_NDJSON="temp_data.jsonl"

          : > "$TEMP_NDJSON"

          while true; do
            echo "Fetching offset=${OFFSET}..."
            URL="${URL_TO_SCRAPE}?api_key=${API_KEY}&_limit=${LIMIT}&_offset=${OFFSET}&_sort=-date_updated&platforms=ps5"

            HTTP_CODE=$(curl -sS -w "%{http_code}" -D "${HEADERS_FILE}" -o "${PAGE_FILE}" "${URL}")

            if [ "${HTTP_CODE}" = "429" ]; then
              RETRY_AFTER=$(grep -i "^Retry-After:" "${HEADERS_FILE}" | awk '{print $2}' | tr -d '\r')
              RETRY_AFTER=${RETRY_AFTER:-60}
              echo "429 Hit. Sleeping ${RETRY_AFTER}s..."
              sleep "$((RETRY_AFTER + 1))"
              continue
            fi

            if [ "${HTTP_CODE}" -lt 200 ] || [ "${HTTP_CODE}" -ge 300 ]; then
              echo "Error ${HTTP_CODE}"; exit 1
            fi

            PAGE_COUNT=$(jq '.data // [] | length' "${PAGE_FILE}")
            echo "  Got ${PAGE_COUNT} items"

            if [ "${PAGE_COUNT}" -eq 0 ]; then break; fi

            jq -c '.data[]' "${PAGE_FILE}" >> "$TEMP_NDJSON"

            if [ "${PAGE_COUNT}" -lt "${LIMIT}" ]; then break; fi
            
            OFFSET=$((OFFSET + LIMIT))
            sleep 0.1
          done

          jq -s 'map({
            id,
            game_id,
            name,
            name_id,
            summary,
            description,
            date_added,
            date_updated,
            date_live,
            visible,
            status,
            dependencies,
            profile_url,
            
            submitted_by: {
              id: .submitted_by.id,
              name_id: .submitted_by.name_id,
              username: .submitted_by.username,
              profile_url: .submitted_by.profile_url,
              profile_img_100x100_url: .submitted_by.avatar.thumb_100x100
            },
            
            modfile: {
              id: .modfile.id,
              mod_id: .modfile.mod_id,
              version: .modfile.version,
              filename: .modfile.filename,
              changelog: .modfile.changelog,
              date_added: .modfile.date_added,
              date_updated: .modfile.date_updated,
              date_scanned: .modfile.date_scanned,
              filesize: .modfile.filesize,
              platforms: .modfile.platforms
            },
            
            logo,
            tags,
            platforms
            
          }) | sort_by(.id)' "$TEMP_NDJSON" > data.json
          
          echo "Finished. data.json size: $(du -h data.json | cut -f1)"

      - name: Commit and push if changed
        id: commit
        run: |
          git config user.name "Git Scraper Bot"
          git config user.email "actions@users.noreply.github.com"
          
          git add data.json
          
          if git diff --staged --quiet; then
            echo "No changes detected."
            echo "data_changed=false" >> $GITHUB_OUTPUT
          else
            echo "Changes detected! Committing..."
            git commit -m "chore: update scraped data [$(date -u)]"
            git push
            echo "data_changed=true" >> $GITHUB_OUTPUT
          fi

  update-changelog:
    needs: scrape
    if: needs.scrape.outputs.data_changed == 'true'
    runs-on: ubuntu-latest
    outputs:
      html_changed: ${{ steps.commit.outputs.html_changed }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history needed for git-history

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: pip install git-history

      - name: Run update_history script
        run: python -m scripts.update_history

      - name: Run generate_html script
        run: python -m scripts.generate_html

      - name: Delete temp mods.db
        run: rm -f mods.db

      - name: Commit and push changes
        id: commit
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add index.html
          
          if git diff --staged --quiet; then
            echo "No HTML changes."
            echo "html_changed=false" >> $GITHUB_OUTPUT
          else
            git commit -m "Update changelog HTML"
            git push
            echo "html_changed=true" >> $GITHUB_OUTPUT
          fi

  deploy:
    needs: update-changelog
    if: always() && (needs.update-changelog.outputs.html_changed == 'true' || inputs.force_deploy)
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: '.'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
